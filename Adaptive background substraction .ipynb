{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from time import sleep\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224  # Default input size for VGG16\n",
    "\n",
    "img_window = (img_width, img_height, 3)\n",
    "batch_size =  256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet', \n",
    "                  include_top=False,\n",
    "                  input_shape=(img_width, img_height, 3))  # 3 = number of channels in RGB pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 6,423,298\n",
      "Trainable params: 6,423,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "# load our best trained model \n",
    "model = tf.keras.models.load_model('model_transferLearningVGG16BS.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide a list in to sebsequences of consecutive integers\n",
    "def subseq_consecutive_nbrs (seq):\n",
    "    \n",
    "    current = []\n",
    "   \n",
    "    for val in seq:\n",
    "        \n",
    "        if current != [] and val != current[-1] + 1:\n",
    "            yield current\n",
    "            \n",
    "            current = []\n",
    "        current += [val]\n",
    "\n",
    "    # Test is only necessary because seq might be empty\n",
    "    if current != []:\n",
    "        yield current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the rectangle containing all the persons in small rectangles. such as one resulted rectangle contains only one person\n",
    "def updated_list_ind(index):\n",
    "\n",
    "    sub_index = subseq_consecutive_nbrs(index)\n",
    "    \n",
    "    list_index = []\n",
    "    prev = -1000\n",
    "\n",
    "    petit = False\n",
    "\n",
    "    for ind in sub_index:\n",
    "              \n",
    "        if len(list_index) == 0:\n",
    "            \n",
    "            if ind[0]+12 > ind[-1]:\n",
    "                petit = True\n",
    "            \n",
    "            list_index.append((ind[0], ind[-1]))\n",
    "            prev = ind[-1]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            if petit :\n",
    "                list_index[-1] = (list(list_index[-1])[0],ind[-1])            \n",
    "                prev = ind[-1]\n",
    "\n",
    "            elif ind[0] <= prev + 30 :\n",
    "                if prev - list(list_index[-1])[0] <= 30:                  \n",
    "                    list_index[-1] = (prev,ind[-1])\n",
    "                else:\n",
    "                    list_index[-1] = (list(list_index[-1])[0],ind[-1])\n",
    "                prev = ind[-1]\n",
    "                \n",
    "            elif  ind[0]+15 <= ind[-1]:\n",
    "                list_index.append((ind[0], ind[-1]))\n",
    "                prev = ind [-1] \n",
    "                \n",
    "            else:\n",
    "                prev = ind [0] \n",
    "            petit = False\n",
    "\n",
    "    return  list_index, petit \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_faces(img):\n",
    "    \n",
    "    img = cv2.resize(img, (img_width, img_height), interpolation = cv2.INTER_NEAREST )\n",
    " \n",
    "    img_tensor = image.img_to_array(img)  # Image data encoded as integers in the 0â€“255 range\n",
    "    img_tensor /= 255.  # Normalize to [0,1] for plt.imshow application\n",
    "\n",
    "    # Extract features\n",
    "    features = conv_base.predict(img_tensor.reshape(1,img_width, img_height, 3))\n",
    "\n",
    "    # Make prediction\n",
    "    try:\n",
    "        prediction = model.predict(features)\n",
    "    except:\n",
    "        prediction = model.predict(features.reshape(1, 7*7*512))\n",
    "        \n",
    "    pred = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    if pred == 1: \n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop the rectangle containing a person in order to get only the coordinates of her face. \"Scale\" representd the ration face/(face+body)\n",
    "\n",
    "def crop_rectangle_faces(p0, p01, frame):\n",
    "    \n",
    "    for scale in [0.2, 0.3, 0.4, 0.7,1]:\n",
    "   \n",
    "        if p0[1] < int(p01[1]*scale): \n",
    "        \n",
    "            # get width almost equal to the lenght \n",
    "            lenght  = int(p01[1]*scale) -  p0[1] \n",
    "            x = max(int((lenght - (p01[0] - p0[0]))/(-2)) - 10, 0)\n",
    "            img = frame[  p0[1] : int(p01[1]*scale),   p0[0]+x : max(p01[0]-x,p0[0]+1 )]\n",
    "            face_label = predict_faces(img)\n",
    "\n",
    "            if face_label:\n",
    "                break\n",
    "\n",
    "    return face_label, (p0[0]+x ,p0[1] ),(max(p01[0]-x,p0[0]+1 ), int(p01[1]*scale)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Subtraction Manual to detect motions. \n",
    "\n",
    "# In the case of AVDIA Dataset. It's about detecting persons and in particular their faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_moving_detection(video_seq):\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(video_seq)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if (cap.isOpened()== False):\n",
    "      print(\"Error opening video stream or file\")\n",
    "    else:\n",
    "        _, background = cap.read()\n",
    "        # Save the first image as background\n",
    "        background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
    "        background = cv2.GaussianBlur(background, (7, 7), 0)\n",
    "        \n",
    "        plt.imshow(background, cmap='gray')\n",
    "    \n",
    "    # Read until video is completed\n",
    "    while (cap.isOpened()):\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "           \n",
    "            diff = cv2.absdiff(gray, background)\n",
    "          \n",
    "            \n",
    "            # Non-Maximum suppression\n",
    "            lim = 0.50*(np.max(diff)-np.min(diff))\n",
    "            \n",
    "            diff[diff<lim]=0\n",
    "           \n",
    "            diff = cv2.GaussianBlur(diff, (21, 21), 0)\n",
    "           \n",
    "            idxs = np.nonzero(diff)\n",
    "           \n",
    "            if len(idxs[0])>1:\n",
    "                \n",
    "                x0 = min(idxs[1])\n",
    "                y0 = min(idxs[0])               \n",
    "                p0 = tuple([x0, y0])\n",
    "                \n",
    "               \n",
    "                x1 = max(idxs[1])\n",
    "                y1 = max(idxs[0])              \n",
    "                p1 = tuple([x1, y1])\n",
    "                \n",
    "            \n",
    "                \n",
    "                index = np.where(~diff[y0:y1, x0:x1].any(axis=0))[0]\n",
    " \n",
    "                final_list_indexes, small = updated_list_ind(index )\n",
    "\n",
    "                if len(index)==0 or small :                    \n",
    "                             \n",
    "                    face_label, r0, r1 = crop_rectangle_faces(p0, p1, frame)\n",
    "                    cv2.rectangle(frame, p0, p1, (255, 0, 0), 1)\n",
    "                    \n",
    "                    if face_label:\n",
    "                        cv2.rectangle(frame, r0, r1, (0, 255, 0), 2)\n",
    "                        \n",
    "                else :\n",
    "                    \n",
    "                    for ind in final_list_indexes :\n",
    "\n",
    "                        p01 = (ind[0]+x0, y1)                       \n",
    "                        cv2.rectangle(frame, p0, p01, (0, 0, 255), 1)  \n",
    "                        \n",
    "                        face_label, r0, r1 = crop_rectangle_faces(p0, p01, frame)\n",
    "                      \n",
    "                        if face_label:\n",
    "                            cv2.rectangle(frame, r0, r1, (0, 255, 0), 2)\n",
    "                                                     \n",
    "\n",
    "                        p0 = (ind[1]+x0, y0)\n",
    "                        \n",
    "                    if len( final_list_indexes ) != 0:\n",
    "                        \n",
    "                        p11 = (ind[1]+x0, y0)   \n",
    "                        cv2.rectangle(frame, p11, p1, (0, 0, 255), 1)\n",
    "                        \n",
    "                        face_label, r0, r1= crop_rectangle_faces(p11, p1, frame)\n",
    "                       \n",
    "                        if face_label:\n",
    "                            cv2.rectangle(frame, r0, r1, (0, 255, 0), 2)\n",
    "                            \n",
    "                            \n",
    "                        \n",
    "                          \n",
    "            # (0, 255, 0) = Green rectangle containing a detected face\n",
    "            # (0, 0, 255) = Red rectangle referes to the existence of TWO or MORE moving persons in the frame. Each rectangle contains one detected person\n",
    "            # (255, 0, 0) = Blue rectangle referes to the existence of ONE moving persons in the frame. It contains the detected person\n",
    "             \n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # if the `q` key is pressed, break from the lop\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "        # Break the loop\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # When everything done, release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\".\\AVDIAR_ALL\\*\")\n",
    "\n",
    "#select the video \n",
    "\n",
    "for folder in folders[4:6]:\n",
    "\n",
    "    video_seq = glob.glob(folder+\"\\Video\\*1.mp4\")  \n",
    "\n",
    "    object_moving_detection(video_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
